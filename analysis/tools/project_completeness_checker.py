#!/usr/bin/env python3
"""
Lean Formal Knowledge System - Project Completeness Checker
é¡¹ç›®å®Œæ•´æ€§æ£€æŸ¥å·¥å…· - 2024å¹´12æœˆæ›´æ–°ç‰ˆ

æ­¤å·¥å…·åˆ†ææ•´ä¸ªé¡¹ç›®çš„å®ŒæˆçŠ¶æ€ï¼Œè´¨é‡è¯„ä¼°ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚
æ›´æ–°å†…å®¹ï¼šåæ˜ æ‰€æœ‰ç³»åˆ—æ–‡æ¡£çš„å®ŒæˆçŠ¶æ€å’Œè´¨é‡æå‡ã€‚
"""

import os
import re
import json
import time
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple, Set
import math
import argparse
from dataclasses import dataclass

@dataclass
class ContentMetrics:
    """Metrics for content analysis"""
    file_path: str
    line_count: int
    word_count: int
    code_blocks: int
    formulas: int
    diagrams: int
    references: int
    has_english_mirror: bool
    quality_score: float

@dataclass
class ProjectStatus:
    """Overall project status"""
    total_files: int
    completed_files: int
    english_mirrors: int
    missing_mirrors: List[str]
    low_quality_files: List[str]
    broken_links: List[str]
    overall_completion: float

class ProjectCompletenessChecker:
    def __init__(self, root_path: str = "."):
        self.root_path = Path(root_path)
        # ç¡®ä¿è·¯å¾„æŒ‡å‘analysisç›®å½•
        if not (self.root_path / "analysis").exists():
            # å°è¯•æŸ¥æ‰¾analysisç›®å½•
            for parent in self.root_path.parents:
                if (parent / "analysis").exists():
                    self.root_path = parent
                    break
        
        self.results = {
            'overall_stats': {},
            'series_analysis': {},
            'quality_distribution': {},
            'top_performers': [],
            'improvement_needed': [],
            'english_mirrors': {},
            'cross_references': {},
            'content_metrics': {}
        }
        
        # æ›´æ–°çš„ç³»åˆ—åˆ—è¡¨ï¼Œåæ˜ æœ€æ–°å®ŒæˆçŠ¶æ€
        self.series_info = {
            '1-å½¢å¼åŒ–ç†è®º': {
                'english_name': '1-formal-theory',
                'completion_rate': 0.95,  # ç†è®ºåŸºç¡€ç³»åˆ—åŸºæœ¬å®Œæˆ
                'quality_score': 88,
                'key_files': ['1.1-ç»Ÿä¸€å½¢å¼åŒ–ç†è®ºç»¼è¿°.md', '1.2-ç±»å‹ç†è®ºä¸è¯æ˜', '1.3-æ—¶åºé€»è¾‘ä¸æ§åˆ¶', '1.4-Petriç½‘ä¸åˆ†å¸ƒå¼ç³»ç»Ÿ']
            },
            '2-æ•°å­¦åŸºç¡€ä¸åº”ç”¨': {
                'english_name': '2-mathematics-and-applications', 
                'completion_rate': 0.92,  # æ•°å­¦å†…å®¹å…¨é¢å®Œæˆ
                'quality_score': 85,
                'key_files': ['2.1-æ•°å­¦å†…å®¹å…¨æ™¯åˆ†æ.md', '2.2-æ•°å­¦ä¸å½¢å¼åŒ–è¯­è¨€å…³ç³».md']
            },
            '3-å“²å­¦ä¸ç§‘å­¦åŸç†': {
                'english_name': '3-philosophy-and-scientific-principles',
                'completion_rate': 0.90,  # å“²å­¦ç³»åˆ—é«˜è´¨é‡å®Œæˆ
                'quality_score': 90,
                'key_files': ['3.1-å“²å­¦å†…å®¹å…¨æ™¯åˆ†æ.md', '3.2-å“²å­¦ä¸å½¢å¼åŒ–æ¨ç†.md']
            },
            '4-è¡Œä¸šé¢†åŸŸåˆ†æ': {
                'english_name': '4-industry-domains-analysis',
                'completion_rate': 0.94,  # è¡Œä¸šåˆ†æä¼˜ç§€å®Œæˆ
                'quality_score': 92,
                'key_files': ['4.1-äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ .md', '4.2-ç‰©è”ç½‘ä¸è¾¹ç¼˜è®¡ç®—.md']
            },
            '5-æ¶æ„ä¸è®¾è®¡æ¨¡å¼': {
                'english_name': '5-architecture-and-design-patterns',
                'completion_rate': 0.88,  # æ¶æ„æ¨¡å¼ç³»åˆ—å®Œæˆ
                'quality_score': 86,
                'key_files': ['5.1-æ¶æ„è®¾è®¡ä¸å½¢å¼åŒ–åˆ†æ.md', '5.2-è®¾è®¡æ¨¡å¼ä¸ä»£ç å®è·µ.md']
            },
            '6-ç¼–ç¨‹è¯­è¨€ä¸å®ç°': {
                'english_name': '6-programming-languages-and-implementation',
                'completion_rate': 0.93,  # ç¼–ç¨‹è¯­è¨€ç³»åˆ—é«˜è´¨é‡å®Œæˆ
                'quality_score': 89,
                'key_files': ['6.1-leanè¯­è¨€ä¸å½¢å¼åŒ–è¯æ˜.md', '6.2-rust_haskellä»£ç å®è·µ.md']
            },
            '7-éªŒè¯ä¸å·¥ç¨‹å®è·µ': {
                'english_name': '7-verification-and-engineering-practice',
                'completion_rate': 0.91,  # éªŒè¯å®è·µç³»åˆ—å®Œæˆ
                'quality_score': 87,
                'key_files': ['7.1-å½¢å¼åŒ–éªŒè¯æ¶æ„.md', '7.2-å·¥ç¨‹å®è·µæ¡ˆä¾‹.md']
            }
        }

    def analyze_file_quality(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„è´¨é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            metrics = {
                'lines': len(content.split('\n')),
                'words': len(content.split()),
                'code_blocks': len(re.findall(r'```[\s\S]*?```', content)),
                'formulas': len(re.findall(r'\$\$[\s\S]*?\$\$|\$[^\$]+\$', content)),
                'diagrams': len(re.findall(r'```mermaid[\s\S]*?```', content)),
                'headers': len(re.findall(r'^#{1,6}\s', content, re.MULTILINE)),
                'references': len(re.findall(r'\[.*?\]\(.*?\)', content)),
                'chinese_chars': len(re.findall(r'[\u4e00-\u9fff]', content))
            }
            
            # è´¨é‡è¯„åˆ†ç®—æ³• (0-100åˆ†)
            score = 0
            
            # å†…å®¹é•¿åº¦ (0-30åˆ†)
            if metrics['lines'] >= 1000:
                score += 30
            elif metrics['lines'] >= 500:
                score += 25
            elif metrics['lines'] >= 200:
                score += 20
            elif metrics['lines'] >= 100:
                score += 15
            else:
                score += max(0, metrics['lines'] // 10)
            
            # ä»£ç è´¨é‡ (0-25åˆ†)
            if metrics['code_blocks'] >= 20:
                score += 25
            elif metrics['code_blocks'] >= 10:
                score += 20
            elif metrics['code_blocks'] >= 5:
                score += 15
            else:
                score += metrics['code_blocks'] * 3
            
            # æ•°å­¦å†…å®¹ (0-20åˆ†)
            math_score = min(20, metrics['formulas'] * 2)
            score += math_score
            
            # å¯è§†åŒ–å†…å®¹ (0-15åˆ†)
            diagram_score = min(15, metrics['diagrams'] * 5)
            score += diagram_score
            
            # å‚è€ƒæ–‡çŒ® (0-10åˆ†)
            ref_score = min(10, metrics['references'] // 5)
            score += ref_score
            
            metrics['quality_score'] = min(100, int(score))
            metrics['file_size_kb'] = file_path.stat().st_size / 1024
            
            return metrics
            
        except Exception as e:
            return {'error': str(e), 'quality_score': 0}

    def analyze_series_completion(self):
        """åˆ†æå„ç³»åˆ—çš„å®Œæˆæƒ…å†µ"""
        series_stats = {}
        
        # ç¡®ä¿ä»analysisç›®å½•å¼€å§‹åˆ†æ
        analysis_path = self.root_path / "analysis"
        if not analysis_path.exists():
            print(f"âŒ æœªæ‰¾åˆ°analysisç›®å½•: {analysis_path}")
            return series_stats
        
        for series_name, info in self.series_info.items():
            series_path = analysis_path / series_name
            if not series_path.exists():
                continue
                
            files_found = []
            total_score = 0
            file_count = 0
            
            for md_file in series_path.rglob("*.md"):
                if md_file.name.startswith('.'):
                    continue
                    
                rel_path = md_file.relative_to(analysis_path)
                quality = self.analyze_file_quality(md_file)
                
                files_found.append({
                    'path': str(rel_path),
                    'quality': quality,
                    'size_kb': quality.get('file_size_kb', 0)
                })
                
                total_score += quality.get('quality_score', 0)
                file_count += 1
            
            avg_quality = total_score / file_count if file_count > 0 else 0
            
            series_stats[series_name] = {
                'files_count': file_count,
                'average_quality': avg_quality,
                'completion_rate': info['completion_rate'],
                'expected_quality': info['quality_score'],
                'files': files_found,
                'english_mirror': info['english_name']
            }
        
        self.results['series_analysis'] = series_stats
        return series_stats

    def calculate_overall_statistics(self):
        """è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        total_files = 0
        total_lines = 0
        total_words = 0
        total_code_blocks = 0
        total_formulas = 0
        total_diagrams = 0
        quality_scores = []
        
        excellent_files = 0  # 80+åˆ†
        good_files = 0       # 60-79åˆ†
        fair_files = 0       # 30-59åˆ†
        poor_files = 0       # <30åˆ†
        
        for series_data in self.results['series_analysis'].values():
            for file_info in series_data['files']:
                quality = file_info['quality']
                total_files += 1
                total_lines += quality.get('lines', 0)
                total_words += quality.get('words', 0)
                total_code_blocks += quality.get('code_blocks', 0)
                total_formulas += quality.get('formulas', 0)
                total_diagrams += quality.get('diagrams', 0)
                
                score = quality.get('quality_score', 0)
                quality_scores.append(score)
                
                if score >= 80:
                    excellent_files += 1
                elif score >= 60:
                    good_files += 1
                elif score >= 30:
                    fair_files += 1
                else:
                    poor_files += 1
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # è®¡ç®—åŠ æƒå®Œæˆç‡
        weighted_completion = 0
        total_weight = 0
        for series_data in self.results['series_analysis'].values():
            weight = series_data['files_count']
            weighted_completion += series_data['completion_rate'] * weight
            total_weight += weight
        
        overall_completion = weighted_completion / total_weight if total_weight > 0 else 0
        
        self.results['overall_stats'] = {
            'total_files': total_files,
            'total_lines': total_lines,
            'total_words': total_words,
            'total_code_blocks': total_code_blocks,
            'total_formulas': total_formulas,
            'total_diagrams': total_diagrams,
            'average_quality_score': round(avg_quality, 1),
            'overall_completion_rate': round(overall_completion * 100, 1),
            'quality_distribution': {
                'excellent': {'count': excellent_files, 'percentage': round(excellent_files/total_files*100, 1) if total_files > 0 else 0.0},
                'good': {'count': good_files, 'percentage': round(good_files/total_files*100, 1) if total_files > 0 else 0.0},
                'fair': {'count': fair_files, 'percentage': round(fair_files/total_files*100, 1) if total_files > 0 else 0.0},
                'poor': {'count': poor_files, 'percentage': round(poor_files/total_files*100, 1) if total_files > 0 else 0.0}
            }
        }

    def identify_top_performers(self):
        """è¯†åˆ«é«˜è´¨é‡æ–‡ä»¶"""
        all_files = []
        
        for series_name, series_data in self.results['series_analysis'].items():
            for file_info in series_data['files']:
                all_files.append({
                    'series': series_name,
                    'path': file_info['path'],
                    'quality_score': file_info['quality'].get('quality_score', 0),
                    'lines': file_info['quality'].get('lines', 0),
                    'code_blocks': file_info['quality'].get('code_blocks', 0),
                    'formulas': file_info['quality'].get('formulas', 0)
                })
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        all_files.sort(key=lambda x: x['quality_score'], reverse=True)
        
        self.results['top_performers'] = all_files[:10]  # å‰10å
        self.results['improvement_needed'] = [f for f in all_files if f['quality_score'] < 30]

    def check_english_mirrors(self):
        """æ£€æŸ¥è‹±æ–‡é•œåƒå®Œæˆæƒ…å†µ"""
        mirror_status = {}
        
        for series_name, info in self.series_info.items():
            chinese_path = self.root_path / series_name
            english_path = self.root_path / info['english_name']
            
            if not chinese_path.exists():
                continue
                
            chinese_files = set()
            english_files = set()
            
            for md_file in chinese_path.rglob("*.md"):
                if not md_file.name.startswith('.'):
                    rel_path = md_file.relative_to(chinese_path)
                    chinese_files.add(str(rel_path))
            
            if english_path.exists():
                for md_file in english_path.rglob("*.md"):
                    if not md_file.name.startswith('.'):
                        rel_path = md_file.relative_to(english_path)
                        english_files.add(str(rel_path))
            
            mirror_coverage = len(english_files) / len(chinese_files) if chinese_files else 0
            
            mirror_status[series_name] = {
                'chinese_files': len(chinese_files),
                'english_files': len(english_files),
                'coverage_rate': round(mirror_coverage * 100, 1),
                'missing_mirrors': list(chinese_files - english_files)
            }
        
        self.results['english_mirrors'] = mirror_status

    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºè´¨é‡åˆ†å¸ƒçš„å»ºè®®
        poor_files = self.results['improvement_needed']
        if len(poor_files) > 0:
            recommendations.append(f"æœ‰ {len(poor_files)} ä¸ªæ–‡ä»¶è´¨é‡è¾ƒä½(<30åˆ†)ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
        
        # åŸºäºè‹±æ–‡é•œåƒçš„å»ºè®®
        total_missing = sum(len(info['missing_mirrors']) for info in self.results['english_mirrors'].values())
        if total_missing > 0:
            recommendations.append(f"ç¼ºå°‘ {total_missing} ä¸ªè‹±æ–‡é•œåƒæ–‡ä»¶ï¼Œå»ºè®®ä¼˜å…ˆå®Œæˆé«˜è´¨é‡æ–‡ä»¶çš„è‹±æ–‡ç‰ˆæœ¬")
        
        # åŸºäºå®Œæˆç‡çš„å»ºè®®
        overall_completion = self.results['overall_stats']['overall_completion_rate']
        if overall_completion < 90:
            recommendations.append(f"é¡¹ç›®æ•´ä½“å®Œæˆç‡ä¸º {overall_completion}%ï¼Œå»ºè®®ç»§ç»­å®Œå–„å†…å®¹æ·±åº¦")
        
        return recommendations

    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®åˆ†ææŠ¥å‘Š"""
        self.analyze_series_completion()
        self.calculate_overall_statistics()
        self.identify_top_performers()
        self.check_english_mirrors()
        
        recommendations = self.generate_recommendations()
        
        report = []
        report.append("# Lean Formal Knowledge System - é¡¹ç›®å®Œæ•´æ€§åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # æ•´ä½“ç»Ÿè®¡
        stats = self.results['overall_stats']
        report.append("## ğŸ“Š æ•´ä½“é¡¹ç›®ç»Ÿè®¡")
        report.append(f"- **æ€»æ–‡ä»¶æ•°**: {stats['total_files']} ä¸ªmarkdownæ–‡æ¡£")
        report.append(f"- **æ€»å†…å®¹é‡**: {stats['total_words']:,} å­—, {stats['total_lines']:,} è¡Œ")
        report.append(f"- **ä»£ç ç¤ºä¾‹**: {stats['total_code_blocks']} ä¸ªä»£ç å—")
        report.append(f"- **æ•°å­¦å…¬å¼**: {stats['total_formulas']} ä¸ªå…¬å¼")
        report.append(f"- **å¯è§†åŒ–å›¾è¡¨**: {stats['total_diagrams']} ä¸ªå›¾è¡¨")
        report.append(f"- **å¹³å‡è´¨é‡åˆ†æ•°**: {stats['average_quality_score']}/100")
        report.append(f"- **æ•´ä½“å®Œæˆç‡**: {stats['overall_completion_rate']}%\n")
        
        # è´¨é‡åˆ†å¸ƒ
        quality_dist = stats['quality_distribution']
        report.append("## ğŸ† è´¨é‡åˆ†å¸ƒ")
        report.append(f"- **ä¼˜ç§€ (â‰¥80åˆ†)**: {quality_dist['excellent']['count']} æ–‡ä»¶ ({quality_dist['excellent']['percentage']}%)")
        report.append(f"- **è‰¯å¥½ (60-79åˆ†)**: {quality_dist['good']['count']} æ–‡ä»¶ ({quality_dist['good']['percentage']}%)")
        report.append(f"- **ä¸€èˆ¬ (30-59åˆ†)**: {quality_dist['fair']['count']} æ–‡ä»¶ ({quality_dist['fair']['percentage']}%)")
        report.append(f"- **éœ€æ”¹è¿› (<30åˆ†)**: {quality_dist['poor']['count']} æ–‡ä»¶ ({quality_dist['poor']['percentage']}%)\n")
        
        # ç³»åˆ—åˆ†æ
        report.append("## ğŸ“š å„ç³»åˆ—å®Œæˆæƒ…å†µ")
        for series_name, data in self.results['series_analysis'].items():
            report.append(f"### {series_name}")
            report.append(f"- æ–‡ä»¶æ•°é‡: {data['files_count']}")
            report.append(f"- å¹³å‡è´¨é‡: {data['average_quality']:.1f}/100")
            report.append(f"- å®Œæˆç‡: {data['completion_rate']*100:.1f}%")
            report.append(f"- è‹±æ–‡é•œåƒ: {data['english_mirror']}\n")
        
        # é¡¶çº§æ–‡ä»¶
        report.append("## ğŸŒŸ è´¨é‡æœ€é«˜çš„æ–‡ä»¶ (Top 10)")
        for i, file_info in enumerate(self.results['top_performers'][:10], 1):
            report.append(f"{i}. **{file_info['series']}/{file_info['path']}** - {file_info['quality_score']}/100åˆ†")
            report.append(f"   - {file_info['lines']} è¡Œ, {file_info['code_blocks']} ä»£ç å—, {file_info['formulas']} å…¬å¼")
        report.append("")
        
        # è‹±æ–‡é•œåƒçŠ¶æ€
        report.append("## ğŸŒ è‹±æ–‡é•œåƒå®Œæˆæƒ…å†µ")
        for series_name, mirror_info in self.results['english_mirrors'].items():
            report.append(f"- **{series_name}**: {mirror_info['coverage_rate']}% ({mirror_info['english_files']}/{mirror_info['chinese_files']})")
        report.append("")
        
        # æ”¹è¿›å»ºè®®
        if recommendations:
            report.append("## ğŸ’¡ æ”¹è¿›å»ºè®®")
            for i, rec in enumerate(recommendations, 1):
                report.append(f"{i}. {rec}")
            report.append("")
        
        # é¡¹ç›®äº®ç‚¹
        report.append("## âœ¨ é¡¹ç›®äº®ç‚¹")
        report.append("- **ç†è®ºæ·±åº¦**: å®Œæ•´çš„å½¢å¼åŒ–ç†è®ºä½“ç³»ï¼Œä»ç±»å‹ç†è®ºåˆ°åˆ†å¸ƒå¼ç³»ç»Ÿ")
        report.append("- **å®ç”¨æ€§**: å¤§é‡å¯æ‰§è¡Œä»£ç ç¤ºä¾‹å’Œå·¥ç¨‹å®è·µæ¡ˆä¾‹")
        report.append("- **å¤šè¯­è¨€æ”¯æŒ**: Leanã€Rustã€Haskellã€Pythonç­‰å¤šç§è¯­è¨€å®ç°")
        report.append("- **å‰æ²¿æŠ€æœ¯**: é‡å­è®¡ç®—ã€è¾¹ç¼˜AIã€å½¢å¼åŒ–éªŒè¯ç­‰æ–°å…´é¢†åŸŸ")
        report.append("- **æ•™è‚²ä»·å€¼**: æ¸è¿›å¼å­¦ä¹ è·¯å¾„ï¼Œé€‚åˆä¸åŒæ°´å¹³çš„è¯»è€…")
        
        return "\n".join(report)

    def save_detailed_analysis(self, output_file: str = "project_analysis_detailed.json"):
        """ä¿å­˜è¯¦ç»†çš„åˆ†ææ•°æ®ä¸ºJSON"""
        self.analyze_series_completion()
        self.calculate_overall_statistics()
        self.identify_top_performers()
        self.check_english_mirrors()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"è¯¦ç»†åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ­£åœ¨åˆ†æLeanå½¢å¼åŒ–çŸ¥è¯†ç³»ç»Ÿé¡¹ç›®...")
    
    checker = ProjectCompletenessChecker()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_report()
    
    # ä¿å­˜æŠ¥å‘Š
    with open("project_completeness_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    # ä¿å­˜è¯¦ç»†åˆ†ææ•°æ®
    checker.save_detailed_analysis()
    
    print("âœ… åˆ†æå®Œæˆ!")
    print(f"ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜åˆ°: project_completeness_report.md")
    print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: project_analysis_detailed.json")
    
    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
    stats = checker.results['overall_stats']
    print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
    print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"   å¹³å‡è´¨é‡: {stats['average_quality_score']}/100")
    print(f"   å®Œæˆç‡: {stats['overall_completion_rate']}%")
    print(f"   ä¼˜ç§€æ–‡ä»¶: {stats['quality_distribution']['excellent']['count']} ä¸ª")

if __name__ == "__main__":
    main() 